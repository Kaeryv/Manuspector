{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorrect-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from toolkit.ascfile import read_asc\n",
    "from glob import glob\n",
    "import numpy as np, matplotlib.pyplot as plt, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alone-cream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1256 entries.\n",
      "There are 1180 files.\n",
      "There are 477 perkin spectrums.\n"
     ]
    }
   ],
   "source": [
    "# We start by listing all the data files, getting some info.\n",
    "raw_entries = glob(\"../raw_data/200to2350nm--Data/**/*\", recursive=True)\n",
    "print(f\"There are {len(raw_entries)} entries.\")\n",
    "files = [ e for e in raw_entries if os.path.isfile(e) ]\n",
    "print(f\"There are {len(files)} files.\")\n",
    "asc_files = [ f for f in files if f.endswith(\".asc\") ]\n",
    "print(f\"There are {len(asc_files)} perkin spectrums.\")\n",
    "asc_files = [ f for f in asc_files if not os.path.basename(f).startswith(\"Empty\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "known-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each file is like\n",
    "# {quantity_token}{species}{country_token}{number?}.Sample.asc\n",
    "tokens = [\"Ag\", \"RTg\", \"TDg\", \"RDg\", \"RD\", \"TD\", \"RT\", \"TTg\", \"TT\", \"A\"]\n",
    "dataset = list()\n",
    "for filename in asc_files:\n",
    "    entry = {}\n",
    "    entry['filename'] = filename\n",
    "    basename = os.path.basename(filename)\n",
    "    # Get the {quantity_token}{species}{country_token}{number?}\n",
    "    entry['name'] = basename.split('.')[0]\n",
    "    match_token = [basename.startswith(t) for t in tokens]\n",
    "    match = match_token.index(True)\n",
    "\n",
    "    if any(match_token):\n",
    "        # Isolate and store {quantity_token} and {species}{country_token}{number?}\n",
    "        entry['token'] = tokens[match] \n",
    "        entry['name'] =entry['name'][len(entry['token']):]\n",
    "        dataset.append(entry)\n",
    "        continue\n",
    "    assert False, f\"This file's token is not identified: {basename}.\"\n",
    "import json\n",
    "with open(\"../data/dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "supported-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract other infos from the name, the species\n",
    "# Or also if it is an unknown species manuscript\n",
    "tokens = [ \"Agneau\", \"Chevre\", \"Veau\", \"ms\" ]\n",
    "from enum import IntEnum\n",
    "class SpectrumKind(IntEnum):\n",
    "    MODERN=0\n",
    "    MANUSCRIPT=1\n",
    "\n",
    "for entry in dataset:\n",
    "    name = entry['name']\n",
    "    match_token = [name.startswith(t) for t in tokens]\n",
    "    if any(match_token):\n",
    "        match = [i for i, x in enumerate(match_token) if x][0]\n",
    "        if match < 3:\n",
    "            entry['kind'] = SpectrumKind.MODERN\n",
    "            entry['animal'] = tokens[match]\n",
    "            other_info = name.replace(tokens[match], '').split(' ')\n",
    "            entry['country'] = other_info[0]\n",
    "            entry['id'] = other_info[1]\n",
    "        else:\n",
    "            # It's a ms\n",
    "            entry['kind'] = SpectrumKind.MANUSCRIPT\n",
    "            entry['id'] = name[2:]\n",
    "            entry['animal'] = \"Unknown\"\n",
    "            entry['country'] = \"Unknown\"\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print(name)\n",
    "        \n",
    "with open(\"../data/dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advance-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the spectrums, put them in database\n",
    "wl = None\n",
    "for entry in dataset:\n",
    "    data = read_asc(entry['filename'])\n",
    "    if wl is None: # Get the wavelength\n",
    "        wl = data[0, :]\n",
    "    \n",
    "    y = data[1, :]\n",
    "    entry['spectrum'] = y.tolist()\n",
    "    \n",
    "    assert np.allclose(wl, data[0,:]), \"Wavelength vector changed!\"\n",
    "    \n",
    "with open(\"../data/dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "configured-sweet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ag 21\n",
      "RTg 21\n",
      "TDg 21\n",
      "RDg 23\n",
      "RD 27\n",
      "TD 24\n",
      "RT 24\n",
      "TTg 21\n",
      "TT 31\n",
      "A 21\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"Ag\", \"RTg\", \"TDg\", \"RDg\", \"RD\", \"TD\", \"RT\", \"TTg\", \"TT\", \"A\"]\n",
    "\n",
    "# Some stats\n",
    "chart = []\n",
    "for t in tokens:\n",
    "    d = list(filter(lambda e: e['token'] == t and e['kind'] == 0, dataset))\n",
    "    print(t, len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broadband-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "from copy import copy\n",
    "def aggregate(data, keys, drops=[]):\n",
    "    import copy\n",
    "    ''' Regroups individuals that share keys values. '''\n",
    "    data_ag = copy.copy(data)\n",
    "    groups = {}\n",
    "    \n",
    "    uniques = set()\n",
    "    for d in data:\n",
    "        values = tuple( d[k] for k in keys )\n",
    "        uniques.add(values)\n",
    "    \n",
    "    for u in uniques:\n",
    "        filtered = copy.deepcopy(list(filter(lambda e: all([ e[k] == v for k, v in zip(keys, u) ]), data)))\n",
    "        filtered_dict = dict()\n",
    "        filtered_dict[\"entries\"] = filtered\n",
    "        filtered_dict[\"commons\"] = dict()\n",
    "        for elem in filtered:\n",
    "            \n",
    "            for k in keys:\n",
    "                filtered_dict[\"commons\"][k] = elem[k]\n",
    "                del elem[k]\n",
    "            for d in drops:\n",
    "                del elem[d]\n",
    "                \n",
    "        groups[\".\".join(map(str, u))] = filtered_dict\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def filter_keyset(data, key, values):\n",
    "    out = {}\n",
    "    for entry in data:\n",
    "        present_values = [ se[key] for se in data[entry][\"entries\"] ]\n",
    "        newentry = {}\n",
    "        if set(values).issubset(set(present_values)):\n",
    "            selentry = copy(data[entry][\"entries\"])\n",
    "            \n",
    "            newentry[\"entries\"] = {}\n",
    "            for i, v in enumerate(selentry):\n",
    "                if v[key] in values:\n",
    "                    newentry[\"entries\"][v[key]] = selentry[i]\n",
    "            \n",
    "            newentry[\"commons\"] = copy(data[entry][\"commons\"])\n",
    "            for v in newentry[\"entries\"]:\n",
    "                del newentry[\"entries\"][v][key]\n",
    "            \n",
    "            out[entry] = newentry\n",
    "    \n",
    "    return out\n",
    "def filter_common_value(data, key, value):\n",
    "    out = {}\n",
    "    for entry in data:\n",
    "        if data[entry][\"commons\"][key] == value:\n",
    "            out[entry] = copy(data[entry])\n",
    "    \n",
    "    return out\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sudden-southwest",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we go from a list of spectrum entries to a list of samples\n",
    "# Each sample shares name, kind, animal, country and id.\n",
    "# So it containes several spectral quantities in \"entries\": \"RD\", \"A\", ...\n",
    "g = aggregate(dataset, [\"name\", \"kind\",\"animal\",\"country\", \"id\"])#, drops=[\"spectrum\"])\n",
    "with open(\"../data/aggregated.json\", \"w\") as f:\n",
    "    json.dump(g, f)\n",
    "from copy import deepcopy\n",
    "g_orig = deepcopy(g)\n",
    "# Get all manuscripts, flesh and grain:\n",
    "g = filter_keyset(deepcopy(g_orig), \"token\", [\"RD\", \"A\"])\n",
    "g = filter_common_value(g, \"kind\", SpectrumKind.MANUSCRIPT)\n",
    "# Get all modern parchments flesh\n",
    "g2 = filter_keyset(deepcopy(g_orig), \"token\", [\"RD\", \"A\"])\n",
    "g2 = filter_common_value(g2, \"kind\", SpectrumKind.MODERN)\n",
    "\n",
    "g3 = filter_keyset(deepcopy(g_orig), \"token\", [\"RDg\", \"Ag\"])\n",
    "g3 = filter_common_value(g3, \"kind\", SpectrumKind.MODERN)\n",
    "g = list(g.values())\n",
    "g.extend(g2.values())\n",
    "g.extend(g3.values())\n",
    "with open(\"../data/by_name.json\", \"w\") as f:\n",
    "    json.dump(g, f)\n",
    "len(list(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "together-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add proteomic animal type, we need human interface using a csv, we create a blacnk csv\n",
    "with open(\"../data/by_name.json\", \"r\") as f:\n",
    "    g = json.load(f)\n",
    "    msnames = []\n",
    "    for i in range(len(g)):\n",
    "        if g[i][\"commons\"][\"kind\"] == SpectrumKind.MANUSCRIPT:\n",
    "            msnames.append(g[i][\"commons\"][\"name\"])\n",
    "    import csv\n",
    "\n",
    "    with open('proteomics_to_fill.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=';',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for n in msnames:\n",
    "            writer.writerow([n]+[\"UnKnown\"]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abc77aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/proteomics_filled.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=';',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for row in reader:\n",
    "        for gg in g:\n",
    "            if gg[\"commons\"][\"name\"] == row[0]:\n",
    "                gg[\"commons\"][\"animal\"] = row[1]\n",
    "with open(\"../data/with_proteomics.json\", \"w\") as f:\n",
    "    json.dump(g, f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae074d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bbc4619a7dbde8f78d9d8f4983711137ac09302ca74d77e1cec41337b43cb69"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
